[ { "title": "Mastering Terraform Validation Pipelines", "url": "/posts/Terraform-Validation-Pipeline/", "categories": "Terraform", "tags": "automation, terraform, azure pipelines, iac, devops, static code analysis, trivy, tflint, semgrep oss, checkov", "date": "2024-04-27 21:00:00 +0200", "snippet": "In this blog post, I want to show you how to create an Azure pipeline that can be used to automatically validate your Terraform code and make suggestions for security, quality and best practices so...", "content": "In this blog post, I want to show you how to create an Azure pipeline that can be used to automatically validate your Terraform code and make suggestions for security, quality and best practices so that you can improve your scrappy code. How? using static code analysis tools! There are a lot out there for Terraform and the major private cloud providers, but I have picked just a few that have made the best impression. For a great comparison of these tools, check out this post.I will use the following tools: Trivy TFLint Semgrep OSS CheckovPipelineNow for the pipeline. First, we need a parameter to pass the working directory where the Terraform root file is located. The default value $(Build.Repository.LocalPath) represents the local path on the agent where your source files will be downloaded. We also define a stage for validation:parameters:- name: workingDirectory type: string default: $(Build.Repository.LocalPath)stages:- stage: TerraformValidationStage displayName: 'Terraform Validation'Next you can add each tool as a seperate step. Note that the execution differs from tool to tool. For Trivy there is a predefined task available in the marketplace. For the others we use bash scripts to install and run the tool.parameters:- name: workingDirectory type: string default: $(Build.Repository.LocalPath)jobs:- job: TerraformValidation displayName: 'Terraform Validate' steps: - checkout: self - task: trivy@1 displayName: 'Trivy' inputs: version: '0.50.1' path: $ exitCode: '0' - script: | curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash tflint --version tflint --init tflint --minimum-failure-severity=error workingDirectory: $ displayName: 'TFLint' - script: | python -m pip install --upgrade pip pip install semgrep semgrep --version semgrep scan --config auto workingDirectory: $ displayName: 'Semgrep OSS' - script: | apt install python3-pip pip3 install checkov checkov --version checkov -d . --hard-fail-on CRITICAL workingDirectory: $ displayName: 'Checkov'For TFLint you need to add a file called .tflint.hcl in the root of your directory which contains the providers you want to validate. In my case I use azurerm.plugin \"azurerm\" { enabled = true version = \"0.26.0\" source = \"github.com/terraform-linters/tflint-ruleset-azurerm\"} To make sure that the pipeline fails only on critical findings or errors, I ran TFLint with --minimum-failure-severity=error and Checkov with --hard-fail-on CRITICAL.And there you go, you have built yourself a pipeline with the major Terraform static code analyzers. If you need all of them, I can’t answer that, but maybe I can make the decision easier with the following sample run to compare the tools.Example RunFollowing I will show you the pipeline in action with a simple terraform setup with a few obvious mistakes to check what each tool has to say.locals { var_with_no_use = \"This is a variable with no use\"}resource \"azurerm_resource_group\" \"example\" { name = \"example-resources\" location = \"switzerlandnorth\"}resource \"azurerm_storage_account\" \"example\" { name = \"storageaccountname\" resource_group_name = azurerm_resource_group.example.name location = azurerm_resource_group.example.location account_tier = \"Standard\" account_replication_type = \"LRS\" min_tls_version = \"TLS1_0\" # Outdated tls version enable_https_traffic_only = false # http enabled}TrivyTrivy, which doesn’t output to the console but to its own “trivy” tab next to the pipeline run summary, warned me about the deprecated tls version and suggested that I enable Secure transfer required, which is recommended by Microsoft:Trivy Example ValidationTFLintTFLint just informed me about the unused variable:TFLint Example ValidationSemgrep OSSWith semgrep we have received again a different suggestion. Firstly, the outdated TLS version again, then that http traffic should be deactivated and also that the storage analytics logs should be used:Semgrep OSS Example ValidationCheckovWe come to the last and most detailed tool, checkov. With 12 failed checks, I can’t even show them in one picture, so below is a list of the things found.Checkov Example Validation Ensure that Storage accounts disallow public access Ensure Storage Account is using the latest version of TLS encryption Ensure that Storage blobs restrict public access Ensure that ‘enable_https_traffic_only’ is enabled Ensure that Storage Accounts use replication Ensure Storage logging is enabled for Queue service for read, write and delete requests Ensure storage account is configured with SAS expiration policy Ensure storage account is configured with private endpoint Ensure soft-delete is enabled on Azure storage account Ensure storage account is configured without blob anonymous access Ensure storage account is not configured with Shared Key authorization Ensure storage for critical data are encrypted with Customer Managed KeyConclusionAs you can see, all the tools vary HEAVILY on the suggestions; some are very general and focus more on syntax and unused declarations, and others more on security aspects and best practices of the resources themselves. You have to decide which one to use based on your needs. You may want to use all of them together, as each tool will have its own suggestions that another tool may not have mentioned, even if things are brought up repeatedly." }, { "title": "YAML Magic: Automating Terraform Like a Pro", "url": "/posts/Terraform-Yaml-Guide/", "categories": "Terraform", "tags": "automation, terraform, yaml, iac, devops", "date": "2024-04-09 21:00:00 +0200", "snippet": "Following scenario: You have created an extremely well-designed Terraform that automates your boring and repetitive task, BUT you are the only one who knows how to take advantage of it.Lets take th...", "content": "Following scenario: You have created an extremely well-designed Terraform that automates your boring and repetitive task, BUT you are the only one who knows how to take advantage of it.Lets take this example Terraform where we want to automate the creation of an EntraID group for each department in your company:data \"azuread_client_config\" \"current\" {}resource \"azuread_group\" \"Sales\" { display_name = \"MySalesGroup\" owners = [data.azuread_client_config.current.object_id] security_enabled = true types = [\"DynamicMembership\"] dynamic_membership { enabled = true rule = \"user.department -eq \\\"Sales\\\"\" }}resource \"azuread_group\" \"HR\" { display_name = \"MyHRGroup\" owners = [data.azuread_client_config.current.object_id] security_enabled = true types = [\"DynamicMembership\"] dynamic_membership { enabled = true rule = \"user.department -eq \\\"HR\\\"\" }}#etc...BeginnerThe above approach is really not a good one, and how should someone who has never worked with terraform create new groups?A better approach would be to define the groups as a local variable:locals { groups = { Sales = { name = \"MySalesGroup\" } HR = { name = \"MyHRGroup\" } #etc... }}This way it is enough to define the resource once and then use for_each to create multiple instances:resource \"azuread_group\" \"department_group\" { for_each = local.groups display_name = each.value.name owners = [data.azuread_client_config.current.object_id] security_enabled = true types = [\"DynamicMembership\"] dynamic_membership { enabled = true rule = \"user.department -eq \\\"${each.key}\\\"\" }}Yaml MagicTo make it even easier, so that even the trainee in the first year of apprenticeship can create new groups in EntraID, just use a Yaml file. Why Yaml you might ask? Well, there is litherally nothing simpler than Yaml - Me 2024If we go with the example above:groups = { Sales = { name = \"MySalesGroup\" } HR = { name = \"MyHRGroup\" } #etc... }A equivalent Yaml file would look like this:Sales: name: \"MySalesGroup\"HR: name: \"MyHRGroup\"Then you just need to decode the Yaml and use it in your resource the same way as with the local variable:locals { groups = yamldecode(file(\"groups.yml\"))}resource \"azuread_group\" \"department_group\" { for_each = local.groups display_name = each.value.name owners = [data.azuread_client_config.current.object_id] security_enabled = true types = [\"DynamicMembership\"] dynamic_membership { enabled = true rule = \"user.department -eq \\\"${each.key}\\\"\" }}Advanced MagicYou can use Yaml to write just about anything. Want to have a list inside a map inside a map? Sure thing:groups: Sales: name: \"MySalesGroup\" members: - luke - leia HR: name: \"MyHRGroup\" members: - han - kyloNow you can create groups with members based on the Yaml file:locals { yaml_file = yamldecode(file(\"groups.yml\")) groups = local.yaml_file.groups}data \"azuread_users\" \"users\" { for_each = local.groups user_principal_names = each.value.members}data \"azuread_client_config\" \"current\" {}resource \"azuread_group\" \"group\" { for_each = local.groups display_name = each.value.name owners = [data.azuread_client_config.current.object_id] security_enabled = true members = data.azuread_users.users[each.key].object_ids} Watch out, as for_each supports only maps and sets of strings!ConclusionAnd that was just the tip of the iceberg of what it could do. I hope I have shown you how you can use the magic of Yaml to your advantage to simplify your life; or more importantly, the lives of others." }, { "title": "Secure Self-Hosting: A Guide to Automatically Pulling and Hosting Your Static Website from GitHub", "url": "/posts/Selfhost-Guide/", "categories": "Self-Hosting", "tags": "docker, caddy, github, nginx-proxy-manager, cloudflare, portainer", "date": "2024-04-01 15:15:00 +0200", "snippet": "In this guide I will show you an easy, lightweight and secure way to host your own website at home without showing your IP to the whole world.The solution consists of the following docker images: ...", "content": "In this guide I will show you an easy, lightweight and secure way to host your own website at home without showing your IP to the whole world.The solution consists of the following docker images: Docker Image Description portainer Management tool for docker caddy Webserver to host the static website nginx proxy manager Easy to use proxy with automatic SSL support watchtower Will automatically update all your container images cloudflare-ddns Used to automatically update the public dns pointer to your public IP Here is an overview of the architecture:Prerequisites Any device to run docker on Own a Domain or be ready to buy one A GitHub Repo with a ready to use static website (at least a index.html)Docker First install docker on your device by following the offical manual. Install Portainer to make your life simpler (you dont need to!) Access Portainer: https://your_ip:9443/Now that docker is prepared you can create all the required containers. I usualy use Stacks inside Portainer but you can do it in you own way. The doc refers only to Stacks.CaddyIn order to “host” my website files inside a GitHub repo and the webserver pulls changes from there I used the caddy-git module from Greenpau and created my own image: https://hub.docker.com/r/greedtp/caddy. The creation of the custom image is done here: https://github.com/GreedTP/caddyThe image can be used as follows:version: \"3.9\"services: caddy: image: greedtp/caddy:latest restart: unless-stopped volumes: - caddy_data:/data - caddy_config:/config environment: - REPO_NAME=wenger.dev - REPO_URL=https://github.com/GreedTP/wenger.dev.gitvolumes: caddy_data: caddy_config:networks: default: external: true name: dmz Note that we use a dedicated network “dmz”. With this it is not required to publish any ports on the containers (except for the proxy). Instead, we can proxy the traffic internaly.Nginx Proxy ManagerCreate the NPM image:version: '3.9'services: npm: image: 'jc21/nginx-proxy-manager:latest' restart: unless-stopped ports: - '80:80' # Public HTTP Port - '443:443' # Public HTTPS Port - '81:81' # Admin Web Port volumes: - ./data:/data - ./letsencrypt:/etc/letsencryptnetworks: default: external: true name: dmzNow you can access the Web interface and create the required proxy host: Login to NPM: https://your_ip:81/ Define username and password Under proxy hosts, create a new entry with your site url which points the name of your created caddy container to port 80: The SSL option will be covered laterNow your website should already be available under: http://your_ip When you commited changes to your website repo, you can pull the latest changes to caddy by just opening: http://your_ip/updateCloudflareTo make your website available to the public and hide your public IP, you can use Cloudflare as a reverse proxy. Sign up on Cloudflare Either register a new domain or add your existing domain to cloudflare Create a new DNS A record which points to your public IP. Make sure that Proxy status is “Proxied”!Port forward and SSL CertificateOn your router, forward ports 80 and 443 to your device IP. You can then request a new certificate for your domain from the SSL tab of your proxy host: If this doesnt work, use the DNS challange option to create a new SSL certificateCongrat! Your site is now public available. But maybe not for long as your public IP might change at some time. To solve this problem I used cloudflare-ddns.Cloudflare-DDNSThis container will automatically adjust the A record to point to your public IP if it changes.Create the config file by following the instructions here: https://github.com/timothymiller/cloudflare-ddnsWhen the config.json file is prepared you can create the docker image:version: '3.9'services: cloudflare-ddns: image: timothyjmiller/cloudflare-ddns:latest security_opt: - no-new-privileges:true network_mode: 'host' volumes: - /data/cloudflare-ddns/config.json:/config.json restart: unless-stopped Make sure the path for the volume matches the location of your config.json file.WatchtowerFinally create the Watchtower image which will update all the other images automatically in your docker:version: \"3.9\"services: watchtower: image: containrrr/watchtower restart: unless-stopped volumes: - /var/run/docker.sock:/var/run/docker.sockConclusionI have shown you a modern way to easily and securely (I would say) host your own website from home.However, this can only be the foundation of your self-hosted infrastructure. Now you can host as many services as you want; all you need to do is deploy a container, create a new entry in NPM, and create a new CNAME entry in Cloudflare." } ]
