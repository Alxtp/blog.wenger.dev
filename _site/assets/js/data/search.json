[
  
  {
    "title": "SSH Authentication Best Practices",
    "url": "/posts/SSH-Setup-Best-Practices/",
    "categories": "Network",
    "tags": "ssh",
    "date": "2025-08-07 21:00:00 +0200",
    "content": "When it comes to securely connecting to your SSH server or using SSH to authenticate to Git, there are several important things to consider. This guide covers generating SSH keys, configuring them on remote servers, disabling password authentication, and using SSH with Git.  Generate SSH Key Pair  To generate a key pair, run the following command:  ssh-keygen -t ed25519 -f ~/.ssh/key_name -C \"comment\"     Give a useful key name and avoid using the default name   Create a password (don’t leave it blank)   Use ed25519 as the key type (more modern and secure than RSA acording to: https://security.stackexchange.com/questions/90077/ssh-key-ed25519-vs-rsa   Include a useful comment (usually your email address)   This command generates a private key (key_name) and a public key (key_name.pub) in the ~/.ssh/directory.  Configure SSH Keys on Remote Server To get the public key onto the remote server (which should have SSH server installed), use this command:  ssh-copy-id -i ~/.ssh/key_name.pub username@hostname   This copies the public key into the ~/.ssh/authorized_keys file on the remote server.  Now, instead of using password authentication to connect to the server with ssh username@hostname you can connect with the key:  ssh -i ~/.ssh/key_name username@hostname   Disable Password Authentication After verifying that the key-based authentication works, you can disable password authentication on the remote server to improve security.  Edit the “/etc/ssh/sshd_config” file and change the line #PasswordAuthentication yes to PasswordAuthentication no. Then restart the SSH service:  sudo systemctl restart ssh   To test this, exit from the SSH session and try to connect with a password:  ssh username@hostname # should not work  ssh -i ~/.ssh/key_name username@hostname # should work   Using ssh authentication for git  When using SSH for authentication with Git, the key can be generated the same way, and then the public key needs to be uploaded to the Git solution like GitHub. Here’s the set up for GitHub:     Generate the SSH key pair as described above   Upload the public key to GitHub   The issue with using custom-named keys is that Git commands like git clone git@github.com:username/repo_name.git can’t specify which key to use, causing SSH to try default key names. This can be seen when running:  ssh -vT git@github.com ... debug1: Authentications that can continue: publickey debug1: Next authentication method: publickey debug1: Will attempt key: /home/username/.ssh/id_rsa debug1: Will attempt key: /home/username/.ssh/id_ecdsa debug1: Will attempt key: /home/username/.ssh/id_ecdsa_sk debug1: Will attempt key: /home/username/.ssh/id_ed25519 debug1: Will attempt key: /home/username/.ssh/id_ed25519_sk debug1: Will attempt key: /home/username/.ssh/id_xmss debug1: Trying private key: /home/username/.ssh/id_rsa debug1: Trying private key: /home/username/.ssh/id_ecdsa debug1: Trying private key: /home/username/.ssh/id_ecdsa_sk debug1: Trying private key: /home/username/.ssh/id_ed25519 debug1: Trying private key: /home/username/.ssh/id_ed25519_sk debug1: Trying private key: /home/username/.ssh/id_xmss debug1: No more authentication methods to try. git@github.com: Permission denied (publickey).   The solution is to create an SSH config file to specify which key to use for GitHub:  Host github.com     HostName github.com     IdentityFile ~/.ssh/github     IdentitiesOnly yes   This configuration will make sure that the key named github is used when connecting to github.com. Now the connection works:  ssh git@github.com Enter passphrase for key '/home/username/.ssh/github': PTY allocation request failed on channel 0 Hi Username! You've successfully authenticated, but GitHub does not provide shell access. Connection to github.com closed.   The same option can be used when connecting to remote server to omit the -i argument on the ssh command. "
  },
  
  {
    "title": "Headless M.2 SSD Setup On Raspberry Pi 5",
    "url": "/posts/Headless-M.2-Setup-On-RPI5/",
    "categories": "IoT",
    "tags": "raspberrypi, m2ssd, nvmessd",
    "date": "2024-07-13 21:00:00 +0200",
    "content": "Learn how to set up your Raspberry Pi 5 to boot from a fast M.2 SSD, all without a graphical interface. Actually, not entirely without, because you will need a GUI to create a bootable SD card or USB stick on another device. But you don’t need to connect your Raspberry to a monitor. This step-by-step guide covers everything from hardware setup to software configuration for a headless setup on your Pi.  You will need:    Another computer to prepare the boot image and SSH into the Pi   Micro SD card or USB stick for initial boot   A Raspberry Pi 5 + M.2 HAT with an installed NVMe SSD   Create Customized OS Image The first step is to download the latest version from the Raspberry Pi Imager and run the installer. Once installed:    Chose Raspberry Pi Device   Operating System -&gt; As we make a headless install choose Raspberry Pi OS Lite (64-bit)   Storage -&gt; Your SD card or USB   When prompted to use OS customization, say “Yes” and complete the entire configuration:      If you have no available WLAN you’ll need to connect it to your LAN by cable.   Under Services, enable SSH and either create a password or generate an SSH key:   After your image has been created, open the drive and copy the file firstrun.sh to your PC as you’ll use it later.  Prepare Raspberry Pi Now you can insert the boot media into your Raspberry Pi. Note that your M.2 hat should not be connected yet, as this could cause your Pi not to boot (was the case on my site).  After a few minutes your Pi should be ready and you should be able to connect to it via SSH with ssh -i .ssh/key_name username@hostname.local or just ssh username@hostname.local if you are using password authentication.  The first thing to check is the bootorder by running sudo rpi-eeprom-config:   The desired value should be 0xf641, which means the following order when searching for a bootable drive:    SD card   USB disk   NVMe SSD   You can edit this value by running bootorder sudo -E rpi-eeprom-config –edit  Now that the bootorder is adjusted to our needs, you can power off the Pi by running sudo poweroff and disconnect it from the power source to connect your M.2 HAT. Then turn it back on and connect again using SSH.  Next, check the name of your M.2 SSD by running lsblk -p  You can see that it is called nvme0n1  Now create a file named firstrun.sh by running nano firstrun.sh, copy the content of the firstrun.sh file you copied earlier, and save it.  OS installation on NVMe SSD  Now that we have prepared the Raspberry Pi and connected the NVMe SSD, we’re ready to install the operating system directly onto the NVMe drive. This process involves using the Raspberry Pi Imager tool on the Raspberry Pi itself. Follow these steps:    sudo apt update   sudo apt install rpi-imager   sudo rpi-imager --cli --first-run-script firstrun.sh https://downloads.raspberrypi.org/raspios_lite_arm64_latest /dev/nvme0n1   Let’s break down the last command:     --cli: Use the command-line interface   --first-run-script firstrun.sh: Specify the firstrun script we created earlier   https://...: The URL for the latest Raspberry Pi OS Lite (64-bit) image   /dev/nvme0n1: The device path for your NVMe SSD (adjust if necessary)   This command will download the latest Raspberry Pi OS Lite (64-bit) image and install it directly onto your NVMe SSD. It will also incorporate the firstrun.sh script we created earlier, which will run on the first boot of the newly installed system.  Now you can power down the Pi by running sudo poweroff and disconnecting your boot media. Then you can power it up again and after a few minutes you can connect to it via SSH in the same way as before!  And there you go, you have successfully installed the headless Raspberry Pi OS on a M.2 SSD without even connecting your PI to anything other than power. "
  },
  
  {
    "title": "Automating Terraform and Provider Updates",
    "url": "/posts/Automating-Terraform-Updates/",
    "categories": "Terraform",
    "tags": "automation, terraform, azure pipelines, iac, azure devops, renovate",
    "date": "2024-06-15 21:00:00 +0200",
    "content": "As infrastructure as code (IaC) becomes increasingly popular, managing Terraform versions and provider dependencies efficiently is crucial. Discover in this blog post how Renovate simplifies managing Terraform versions and providers in your Git repository. Renovate allows some very specialized and complex setups, as you can configure a lot and it supports a lot of tools, but I want to show you how easy it is to set it up for Terraform.  In order for Renovate to manage your Terraform and Provider versions inside a Azure DevOps repository, you need these two things:    Pipeline that runs Renovate on a schedule.   A Renovate config file which tells Renovate what to do.   Pipeline  First you create the Azure pipeline, which is actually very simple. It will run every day at 03:00 and just set up the git user mail and name and then run Renovate with some environment variables:  schedules:   - cron: '0 3 * * *'     displayName: 'Every day at 3am (UTC)'     branches:       include: [main]     always: true  trigger: none  pool:   vmImage: ubuntu-latest  steps:   - bash: |       git config --global user.email 'bot@renovateapp.com'       git config --global user.name 'Renovate Bot'       npx renovate     env:       RENOVATE_PLATFORM: azure       RENOVATE_ENDPOINT: $(System.CollectionUri)       RENOVATE_TOKEN: $(System.AccessToken)       GITHUB_COM_TOKEN: $(GITHUB_TOKEN)       RENOVATE_REPOSITORIES: \"['PROJECT_NAME/REPO_NAME']\"       LOG_LEVEL: DEBUG   The only two things you need to do is to reference one or more projects and repositories in which you want to run Renovate with the RENOVATE_REPOSITORIES env variable, and create a GitHub personal access token (read-only) that will be used to fetch changelogs for repositories.  I stored this token as a secret pipeline variable:       Make sure the Azure DevOps build service called PROJECT_NAME Build Service (ORGANISATION_NAME) has permission to contribute to the specified repositories.      Although it is not required, for troubleshooting at the beginning of your setup, it makes sence to turn on logs with LOG_LEVEL: DEBUG   Renovate Config  Once you have created the pipeline, you need to create a configuration file called renovate.json and put it in the root of each of your repositories that you want to manage with Renovate. The filename, format, and configuration may be different for each repo - see Renovate Configuration Options for more information. Inside this file you can configure a lot of things, but in this guide I’ll focus on Terraform. Therefore, a basic setup would look like this {   \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",   \"extends\": [     \"config:recommended\"   ],   \"enabledManagers\": [     \"terraform\"   ]   }   It uses the recommended configuration and enables the Terraform Manager which, according to the Renovate Manager docs, includes automatic updates for these dependencies:    terraform   terraform-version   terragrunt   terragrunt-version   tflint-plugin   And there you go. Renovate will check for new Terraform and provider versions and automatically create a pull request for you to approve:      Of course, it is recommended to run a validation on such a PR to check for breaking changes in your infrastructure before merging it into main.   Before I come to the end, I want to show another scenario where some custom configuration was required. I had the problem that in my pipeline which is running Terraform, the Terraform version was specified as a parameter. In such a case, Renovate does not know that this parameter represents a Terraform version and therefore will not update it. BUT you can tell Renovate to treat it as a Terraform version. First, let us look at the parameters of the mentioned pipeline: parameters: - name: Environment   type: string   default: test   values:   - test   - prod - name: TerraformVersion   type: string   default: 1.8.5 - name: TerraformArguments   type: string   default: \" \"   As you can see, the Terraform version is specified as a default value of the TerraformVersion parameter. With a customManager entry in the renovate.json and some regex you can tell Renovate in which file, at which position, to update the Terraform version: {   \"$schema\": \"https://docs.renovatebot.com/renovate-schema.json\",   \"extends\": [     \"config:recommended\"   ],   \"enabledManagers\": [     \"terraform\",     \"custom.regex\"   ],   \"customManagers\": [     {       \"customType\": \"regex\",       \"fileMatch\": [         \"^azure-pipelines\\\\.?[a-z0-9_.-]*\\\\.yml$\"       ],       \"matchStrings\": [         \"- name: TerraformVersion\\\\n  type: string\\\\n  default: (?&lt;currentValue&gt;.*)\"       ],       \"depNameTemplate\": \"hashicorp/terraform\",       \"depTypeTemplate\": \"required_version\",       \"datasourceTemplate\": \"github-releases\",       \"versioningTemplate\": \"hashicorp\"     }   ] }   Conclusion In summary, Renovate bridges the gap between complexity and simplicity, as it is very easy to use, but can be very complex depending on your needs. It makes Terraform version updates a manageable and efficient process. "
  },
  
  {
    "title": "Create Terraform Validation Pipelines",
    "url": "/posts/Terraform-Validation-Pipeline/",
    "categories": "Terraform",
    "tags": "automation, terraform, azure pipelines, iac, devops, static code analysis, trivy, tflint, semgrep oss, checkov",
    "date": "2024-04-27 21:00:00 +0200",
    "content": "In this blog post, I want to show you how to create an Azure pipeline that can be used to automatically validate your Terraform code and make suggestions for security, quality and best practices so that you can improve your scrappy code. How? using static code analysis tools! There are a lot out there for Terraform and the major private cloud providers, but I have picked just a few that have made the best impression. For a great comparison of these tools, check out this post.  I will use the following tools:    Trivy   TFLint   Semgrep OSS   Checkov   Pipeline  Now for the pipeline. First, we need a parameter to pass the working directory where the Terraform root file is located. The default value $(Build.Repository.LocalPath) represents the local path on the agent where your source files will be downloaded. We also define a stage for validation:  parameters: - name: workingDirectory   type: string   default: $(Build.Repository.LocalPath)  stages: - stage: TerraformValidationStage   displayName: 'Terraform Validation'   Next you can add each tool as a seperate step. Note that the execution differs from tool to tool. For Trivy there is a predefined task available in the marketplace. For the others we use bash scripts to install and run the tool.  parameters: - name: workingDirectory   type: string   default: $(Build.Repository.LocalPath)  jobs: - job: TerraformValidation   displayName: 'Terraform Validate'   steps:   - checkout: self       - task: trivy@1     displayName: 'Trivy'     inputs:     version: '0.50.1'     path: $     exitCode: '0'      - script: |       curl -s https://raw.githubusercontent.com/terraform-linters/tflint/master/install_linux.sh | bash        tflint --version       tflint --init       tflint --minimum-failure-severity=error     workingDirectory: $     displayName: 'TFLint'      - script: |       python -m pip install --upgrade pip       pip install semgrep        semgrep --version       semgrep scan --config auto     workingDirectory: $     displayName: 'Semgrep OSS'      - script: |       apt install python3-pip       pip3 install checkov        checkov --version       checkov -d . --hard-fail-on CRITICAL     workingDirectory: $     displayName: 'Checkov'   For TFLint you need to add a file called .tflint.hcl in the root of your directory which contains the providers you want to validate. In my case I use azurerm. plugin \"azurerm\" {   enabled = true   version = \"0.26.0\"   source  = \"github.com/terraform-linters/tflint-ruleset-azurerm\" }      To make sure that the pipeline fails only on critical findings or errors, I ran TFLint with --minimum-failure-severity=error and Checkov with --hard-fail-on CRITICAL.   And there you go, you have built yourself a pipeline with the major Terraform static code analyzers. If you need all of them, I can’t answer that, but maybe I can make the decision easier with the following sample run to compare the tools.  Example Run  Following I will show you the pipeline in action with a simple terraform setup with a few obvious mistakes to check what each tool has to say.  locals {   var_with_no_use = \"This is a variable with no use\" }  resource \"azurerm_resource_group\" \"example\" {   name     = \"example-resources\"   location = \"switzerlandnorth\" }  resource \"azurerm_storage_account\" \"example\" {   name                      = \"storageaccountname\"   resource_group_name       = azurerm_resource_group.example.name   location                  = azurerm_resource_group.example.location   account_tier              = \"Standard\"   account_replication_type  = \"LRS\"   min_tls_version           = \"TLS1_0\" # Outdated tls version   enable_https_traffic_only = false # http enabled }   Trivy  Trivy, which doesn’t output to the console but to its own “trivy” tab next to the pipeline run summary, warned me about the deprecated tls version and suggested that I enable Secure transfer required, which is recommended by Microsoft:   Trivy Example Validation  TFLint  TFLint just informed me about the unused variable:   TFLint Example Validation  Semgrep OSS  With semgrep we have received again a different suggestion. Firstly, the outdated TLS version again, then that http traffic should be deactivated and also that the storage analytics logs should be used:   Semgrep OSS Example Validation  Checkov  We come to the last and most detailed tool, checkov. With 12 failed checks, I can’t even show them in one picture, so below is a list of the things found.   Checkov  Example Validation     Ensure that Storage accounts disallow public access   Ensure Storage Account is using the latest version of TLS encryption   Ensure that Storage blobs restrict public access   Ensure that ‘enable_https_traffic_only’ is enabled   Ensure that Storage Accounts use replication   Ensure Storage logging is enabled for Queue service for read, write and delete requests   Ensure storage account is configured with SAS expiration policy   Ensure storage account is configured with private endpoint   Ensure soft-delete is enabled on Azure storage account   Ensure storage account is configured without blob anonymous access   Ensure storage account is not configured with Shared Key authorization   Ensure storage for critical data are encrypted with Customer Managed Key   Conclusion  As you can see, all the tools vary HEAVILY on the suggestions; some are very general and focus more on syntax and unused declarations, and others more on security aspects and best practices of the resources themselves. You have to decide which one to use based on your needs. You may want to use all of them together, as each tool will have its own suggestions that another tool may not have mentioned, even if things are brought up repeatedly. "
  },
  
  {
    "title": "Automate Terraform with Yaml",
    "url": "/posts/Terraform-Yaml-Guide/",
    "categories": "Terraform",
    "tags": "automation, terraform, yaml, iac, devops",
    "date": "2024-04-09 21:00:00 +0200",
    "content": "Following scenario: You have created an extremely well-designed Terraform that automates your boring and repetitive task, BUT you are the only one who knows how to take advantage of it.  Lets take this example Terraform where we want to automate the creation of an EntraID group for each department in your company:  data \"azuread_client_config\" \"current\" {}  resource \"azuread_group\" \"Sales\" {   display_name     = \"MySalesGroup\"   owners           = [data.azuread_client_config.current.object_id]   security_enabled = true   types            = [\"DynamicMembership\"]    dynamic_membership {     enabled = true     rule    = \"user.department -eq \\\"Sales\\\"\"   } }  resource \"azuread_group\" \"HR\" {   display_name     = \"MyHRGroup\"   owners           = [data.azuread_client_config.current.object_id]   security_enabled = true   types            = [\"DynamicMembership\"]    dynamic_membership {     enabled = true     rule    = \"user.department -eq \\\"HR\\\"\"   } }  #etc...   Beginner The above approach is really not a good one, and how should someone who has never worked with terraform create new groups? A better approach would be to define the groups as a local variable:  locals {   groups = {     Sales = {       name = \"MySalesGroup\"     }     HR = {       name = \"MyHRGroup\"     }     #etc...   } }   This way it is enough to define the resource once and then use for_each to create multiple instances: resource \"azuread_group\" \"department_group\" {   for_each = local.groups    display_name     = each.value.name   owners           = [data.azuread_client_config.current.object_id]   security_enabled = true   types            = [\"DynamicMembership\"]    dynamic_membership {     enabled = true     rule    = \"user.department -eq \\\"${each.key}\\\"\"   } }   Yaml Magic To make it even easier, so that even the trainee in the first year of apprenticeship can create new groups in EntraID, just use a Yaml file.  Why Yaml you might ask?     Well, there is litherally nothing simpler than Yaml - Me 2024   If we go with the example above: groups = {     Sales = {       name = \"MySalesGroup\"     }     HR = {       name = \"MyHRGroup\"     }     #etc...   }  A equivalent Yaml file would look like this: Sales:   name: \"MySalesGroup\" HR:   name: \"MyHRGroup\"   Then you just need to decode the Yaml and use it in your resource the same way as with the local variable: locals {   groups = yamldecode(file(\"groups.yml\")) }  resource \"azuread_group\" \"department_group\" {   for_each = local.groups    display_name     = each.value.name   owners           = [data.azuread_client_config.current.object_id]   security_enabled = true   types            = [\"DynamicMembership\"]    dynamic_membership {     enabled = true     rule    = \"user.department -eq \\\"${each.key}\\\"\"   } }   Advanced Magic You can use Yaml to write just about anything. Want to have a list inside a map inside a map? Sure thing: groups:   Sales:     name: \"MySalesGroup\"     members:       - luke       - leia   HR:     name: \"MyHRGroup\"     members:       - han       - kylo   Now you can create groups with members based on the Yaml file: locals {   yaml_file = yamldecode(file(\"groups.yml\"))   groups    = local.yaml_file.groups }  data \"azuread_users\" \"users\" {   for_each = local.groups    user_principal_names = each.value.members }  data \"azuread_client_config\" \"current\" {}  resource \"azuread_group\" \"group\" {   for_each = local.groups    display_name     = each.value.name   owners           = [data.azuread_client_config.current.object_id]   security_enabled = true   members          = data.azuread_users.users[each.key].object_ids }      Watch out, as for_each supports only maps and sets of strings!   Conclusion And that was just the tip of the iceberg of what it could do. I hope I have shown you how you can use the magic of Yaml to your advantage to simplify your life; or more importantly, the lives of others. "
  },
  
  {
    "title": "Automated Hosting of Static Websites with Docker",
    "url": "/posts/Selfhost-Guide/",
    "categories": "Self-Hosting",
    "tags": "docker, caddy, github, nginx-proxy-manager, cloudflare, portainer",
    "date": "2024-04-01 15:15:00 +0200",
    "content": "In this guide I will show you an easy, lightweight and secure way to host your own website at home without showing your IP to the whole world. The solution consists of the following docker images:                 Docker Image       Description                       portainer       Management tool for docker                 caddy       Webserver to host the static website                 nginx proxy manager       Easy to use proxy with automatic SSL support                 watchtower       Will automatically update all your container images                 cloudflare-ddns       Used to automatically update the public dns pointer to your public IP           Here is an overview of the architecture:     Prerequisites    Any device to run docker on   Own a Domain or be ready to buy one   A GitHub Repo with a ready to use static website (at least a index.html)   Docker    First install docker on your device by following the offical manual.   Install Portainer to make your life simpler (you dont need to!)   Access Portainer: https://your_ip:9443/   Now that docker is prepared you can create all the required containers. I usualy use Stacks inside Portainer but you can do it in you own way. The doc refers only to Stacks.  Caddy In order to “host” my website files inside a GitHub repo and the webserver pulls changes from there I used the caddy-git module from Greenpau and created my own image: https://hub.docker.com/r/greedtp/caddy.     The creation of the custom image is done here: https://github.com/GreedTP/caddy   The image can be used as follows: version: \"3.9\"  services:   caddy:     image: greedtp/caddy:latest     restart: unless-stopped     volumes:       - caddy_data:/data       - caddy_config:/config     environment:       - REPO_NAME=wenger.dev       - REPO_URL=https://github.com/GreedTP/wenger.dev.git  volumes:   caddy_data:   caddy_config:  networks:   default:     external: true     name: dmz     Note that we use a dedicated network “dmz”. With this it is not required to publish any ports on the containers (except for the proxy). Instead, we can proxy the traffic internaly.   Nginx Proxy Manager Create the NPM image: version: '3.9' services:   npm:     image: 'jc21/nginx-proxy-manager:latest'     restart: unless-stopped     ports:       - '80:80' # Public HTTP Port       - '443:443' # Public HTTPS Port       - '81:81' # Admin Web Port     volumes:       - ./data:/data       - ./letsencrypt:/etc/letsencrypt  networks:   default:     external: true     name: dmz  Now you can access the Web interface and create the required proxy host:    Login to NPM: https://your_ip:81/   Define username and password   Under proxy hosts, create a new entry with your site url which points the name of your created caddy container to port 80:        The SSL option will be covered later   Now your website should already be available under: http://your_ip     When you commited changes to your website repo, you can pull the latest changes to caddy by just opening: http://your_ip/update   Cloudflare To make your website available to the public and hide your public IP, you can use Cloudflare as a reverse proxy.     Sign up on Cloudflare   Either register a new domain or add your existing domain to cloudflare   Create a new DNS A record which points to your public IP. Make sure that Proxy status is “Proxied”!     Port forward and SSL Certificate On your router, forward ports 80 and 443 to your device IP. You can then request a new certificate for your domain from the SSL tab of your proxy host:       If this doesnt work, use the DNS challange option to create a new SSL certificate   Congrat! Your site is now public available. But maybe not for long as your public IP might change at some time. To solve this problem I used cloudflare-ddns.  Cloudflare-DDNS This container will automatically adjust the A record to point to your public IP if it changes. Create the config file by following the instructions here: https://github.com/timothymiller/cloudflare-ddns  When the config.json file is prepared you can create the docker image: version: '3.9' services:   cloudflare-ddns:     image: timothyjmiller/cloudflare-ddns:latest     security_opt:       - no-new-privileges:true     network_mode: 'host'     volumes:       - /data/cloudflare-ddns/config.json:/config.json     restart: unless-stopped      Make sure the path for the volume matches the location of your config.json file.   Watchtower Finally create the Watchtower image which will update all the other images automatically in your docker: version: \"3.9\" services:   watchtower:     image: containrrr/watchtower     restart: unless-stopped     volumes:       - /var/run/docker.sock:/var/run/docker.sock   Conclusion I have shown you a modern way to easily and securely (I would say) host your own website from home.  However, this can only be the foundation of your self-hosted infrastructure. Now you can host as many services as you want; all you need to do is deploy a container, create a new entry in NPM, and create a new CNAME entry in Cloudflare. "
  }
  
]

